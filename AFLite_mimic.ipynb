{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFLite_mimic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAgQ23/KmxG45eUPir8zMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokulanv/Data_Shapley_NLP/blob/master/AFLite_mimic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnjGipCJfLd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMrOmhUbtXUf",
        "colab_type": "text"
      },
      "source": [
        "### One time config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuT1mkPNCzrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, labels = load_breast_cancer(return_X_y = True)\n",
        "train_size = 200\n",
        "predictability_score = [0.0] * data.shape[0]\n",
        "m = 10\n",
        "\n",
        "classifiers = [\n",
        "               LogisticRegression(),\n",
        "               LinearSVC(),\n",
        "               KNeighborsClassifier(),\n",
        "               DecisionTreeClassifier(),\n",
        "               RandomForestClassifier(),\n",
        "               GradientBoostingClassifier()\n",
        "               ]\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('normalizer', StandardScaler()),\n",
        "                 ('clf', None)\n",
        "])\n",
        "\n",
        "#Check if all data points are unique\n",
        "unique_rows = np.unique(data, axis=0)\n",
        "assert unique_rows.shape == data.shape\n",
        "\n",
        "# TODO: Remove duplicate points, if any\n",
        "\n",
        "\n",
        "# check if indexed properly\n",
        "test_indices = np.random.randint(0,data.shape[0], 10)\n",
        "for idx in test_indices:\n",
        "    assert data2idx[tuple(data[idx])] == idx\n",
        "\n",
        "data2idx = dict({tuple(v):k for k,v in enumerate(data)})\n",
        "idx2count_sampled_points = dict({k: 0 for k in range(data.shape[0] + 1)})\n",
        "idx2count_classified_correctly = dict({k: 0 for k in range(data.shape[0] + 1)})\n",
        "\n",
        "assert sum(idx2count_sampled_points[k] for k in idx2count_sampled_points.keys()) == 0\n",
        "assert sum(idx2count_classified_correctly[k] for k in idx2count_sampled_points.keys()) == 0\n",
        "assert len(idx2count_classified_correctly) == len(idx2count_sampled_points)\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJKxSIT_tZbh",
        "colab_type": "text"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LijUo2_jmoM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_train_test_split(train_size: int) -> tuple:\n",
        "    return train_test_split(data, labels, train_size = train_size)\n",
        "\n",
        "def update_test_datapoints_count(Xtest: np.ndarray, idx2count: dict, indices: list = None) -> None:\n",
        "    if indices:\n",
        "        # update only given indices\n",
        "        Xtest = Xtest[indices]\n",
        "    [idx2count.__setitem__(data2idx[tuple(dp)], idx2count[data2idx[tuple(dp)]] + 1)  for dp in Xtest]\n",
        "\n",
        "update_test_datapoints_count(Xtest, idx2count_sampled_points)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDA_WMcRJFNl",
        "colab_type": "text"
      },
      "source": [
        "### Models pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33c89u1PHsSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# AFLite random sampling\n",
        "for i in range(m):\n",
        "    Xtrain, Xtest, Ytrain, Ytest = get_random_train_test_split(train_size)\n",
        "\n",
        "    for clf in classifiers:\n",
        "        pipe.set_params(clf = clf)\n",
        "        # scores = cross_validate(pipe, data, labels)\n",
        "        grid_search = GridSearchCV(pipe, {}, n_jobs=4)\n",
        "        grid_search.fit(Xtrain, Ytrain)\n",
        "\n",
        "        correct_pred_indices = [i for i in range(len(Xtest)) if grid_search.predict(np.expand_dims(Xtest[i], axis=0)) == Ytest[i]]\n",
        "        update_test_datapoints_count(Xtest, idx2count_classified_correctly, indices = correct_pred_indices)\n",
        "\n",
        "    for i in range(len(predictability_score)):\n",
        "        predictability_score[i] += idx2count_classified_correctly[i]/(idx2count_sampled_points[i] + 1e-9)\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MowSQCtmsj6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b255e963-0971-4b34-fbf8-51b4523941be"
      },
      "source": [
        "print('#data_points < 100 test score threshold = ', str(np.sum(np.asarray(predictability_score) < 100)) )"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#data_points < 100 test score threshold =  19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OcwR76AJJ7p",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irgRi2DmJJhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94d41536-a22a-45e0-cae7-4f2712010b9c"
      },
      "source": [
        "clf = LogisticRegression(random_state=0, n_jobs=4, max_iter=100)\n",
        "clf.fit(Xtrain, Ytrain)\n",
        "clf.score(Xtrain, Ytrain), clf.score(Xtest, Ytest)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.97, 0.8780487804878049)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}