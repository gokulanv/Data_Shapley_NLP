{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFLite_mimic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8HAi1l917H5S1YHxjGWQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokulanv/Data_Shapley_NLP/blob/master/AFLite_mimic_covtype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnjGipCJfLd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "407ef0aa-7e0b-4915-bc43-e4075189e113"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer, fetch_covtype\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMrOmhUbtXUf",
        "colab_type": "text"
      },
      "source": [
        "### One time config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuT1mkPNCzrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4261406-53aa-4ee9-fec6-29f5de6c05c7"
      },
      "source": [
        "data, labels = fetch_covtype(return_X_y = True)\n",
        "data = data[:30000]\n",
        "labels = labels[:30000]\n",
        "train_size = 15000\n",
        "predictability_score = [0.0] * data.shape[0]\n",
        "m = 10\n",
        "\n",
        "classifiers = [\n",
        "               LogisticRegression(n_jobs=4, C=1.0, max_iter=5000, random_state=666),\n",
        "               OneVsRestClassifier(SVC(random_state=666), n_jobs=4)\n",
        "               ]\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('normalizer', StandardScaler()),\n",
        "                 ('clf', None)\n",
        "])\n",
        "\n",
        "#Check if all data points are unique\n",
        "unique_rows = np.unique(data, axis=0)\n",
        "assert unique_rows.shape == data.shape\n",
        "\n",
        "# TODO: Remove duplicate points, if any\n",
        "\n",
        "data2idx = dict({tuple(v):k for k,v in enumerate(data)})\n",
        "idx2count_sampled_points = dict({k: 0 for k in range(data.shape[0] + 1)})\n",
        "idx2count_classified_correctly = dict({k: 0 for k in range(data.shape[0] + 1)})\n",
        "\n",
        "# check if indexed properly\n",
        "test_indices = np.random.randint(0,data.shape[0], 10)\n",
        "for idx in test_indices:\n",
        "    assert data2idx[tuple(data[idx])] == idx\n",
        "\n",
        "assert sum(idx2count_sampled_points[k] for k in idx2count_sampled_points.keys()) == 0\n",
        "assert sum(idx2count_classified_correctly[k] for k in idx2count_sampled_points.keys()) == 0\n",
        "assert len(idx2count_classified_correctly) == len(idx2count_sampled_points)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://ndownloader.figshare.com/files/5976039\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHbJVJ5r82i_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98f0cbe7-9c80-4be8-c968-d124b91945fd"
      },
      "source": [
        "train_size = 15000\n",
        "np.unique(labels[:10000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4, 5, 6, 7], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJKxSIT_tZbh",
        "colab_type": "text"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LijUo2_jmoM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_train_test_split(train_size: int) -> tuple:\n",
        "    return train_test_split(data, labels, train_size = train_size)\n",
        "\n",
        "def update_test_datapoints_count(Xtest: np.ndarray, idx2count: dict, indices: list = None) -> None:\n",
        "    if indices:\n",
        "        # update only given indices\n",
        "        Xtest = Xtest[indices]\n",
        "    [idx2count.__setitem__(data2idx[tuple(dp)], idx2count[data2idx[tuple(dp)]] + 1)  for dp in Xtest]\n",
        "\n",
        "# update_test_datapoints_count(Xtest, idx2count_sampled_points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDA_WMcRJFNl",
        "colab_type": "text"
      },
      "source": [
        "### Models pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33c89u1PHsSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "aa1e541e-8694-442c-f856-51c6452cd384"
      },
      "source": [
        "# AFLite random sampling\n",
        "for i in range(m):\n",
        "    Xtrain, Xtest, Ytrain, Ytest = get_random_train_test_split(train_size)\n",
        "\n",
        "    for clf in classifiers:\n",
        "        pipe.set_params(clf = clf)\n",
        "        # scores = cross_validate(pipe, data, labels)\n",
        "        grid_search = GridSearchCV(pipe, {}, n_jobs=4)\n",
        "        grid_search.fit(Xtrain, Ytrain)\n",
        "\n",
        "        correct_pred_indices = [i for i in range(len(Xtest)) if grid_search.predict(np.expand_dims(Xtest[i], axis=0)) == Ytest[i]]\n",
        "        update_test_datapoints_count(Xtest, idx2count_classified_correctly, indices = correct_pred_indices)\n",
        "\n",
        "    for i in range(len(predictability_score)):\n",
        "        predictability_score[i] += idx2count_classified_correctly[i]/(idx2count_sampled_points[i] + 1e-9)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MowSQCtmsj6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19c3334e-5df9-4432-8013-dd41952b6cf7"
      },
      "source": [
        "print('#data_points < 100 test score threshold = ', str(np.sum(np.asarray(predictability_score) < 100)) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#data_points < 100 test score threshold =  4432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3rDd8I81RtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b2943b5-2d1e-40ce-bc55-2bd074baa28b"
      },
      "source": [
        "np.sum(np.asarray(predictability_score) == 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4432"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OcwR76AJJ7p",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irgRi2DmJJhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b260a26-cab7-462c-f9ac-aff777af9643"
      },
      "source": [
        "a = np.array([[10, 7, 4], [3, 2, 1]])\n",
        "q = [25,50,75, 100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.25, 3.5, 6.25, 10.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}